{
    "name": {
        "zh": "AI意识观察",
        "en": "AI Consciousness Watch"
    },
    "description": {
        "zh": "AI意识的多层次评估框架",
        "en": "A multi-level evaluation framework for AI consciousness"
    },
    "version": "0.1",
    "author": "Jinge Wang",
    "license": "MIT",
    "Levels": [
        {
            "id": "Level 1",
            "title": {
                "zh": "哲学",
                "en": "Philosophy"
            },
            "subtitle": {
                "zh": "意识的本质与前提",
                "en": "The Nature and Prerequisites of Consciousness"
            },
            "core_question": {
                "zh": "该AI系统在概念上是否满足了成为\"意识主体\"的基本前提？",
                "en": "Does this AI system conceptually meet the basic prerequisites to become a 'conscious subject'?"
            },
            "metrics": [
                {
                    "id": "P-1",
                    "name": {
                        "zh": "现象意识 (Phenomenal Consciousness)",
                        "en": "Phenomenal Consciousness"
                    },
                    "description": {
                        "zh": "系统是否具备主观的第一人称体验，即\"感受（qualia）\"——对内部或外部刺激产生的不可言喻的感受质量。",
                        "en": "Whether the system has subjective first-person experience, i.e., 'qualia' - the ineffable feeling quality produced by internal or external stimuli."
                    },
                    "weight": "50%",
                    "papers": [
                        {
                            "title": "Garrido & Lumbreras (2022), On the independence between phenomenal consciousness and computational intelligence",
                            "url": "https://arxiv.org/abs/2208.02187",
                            "core_argument": {
                                "zh": "提出现象意识与计算智能相互独立的观点：机器虽然可能具备极高的计算智能，但这并不意味着它们拥有感质（qualia）。论文认为，即使机器在行为上完全模拟人类的意识表现，也无法证明其真正具备了主观体验。",
                                "en": "Proposes that phenomenal consciousness and computational intelligence are independent: although machines may possess extremely high computational intelligence, this does not mean they have qualia. The paper argues that even if machines perfectly simulate human conscious behavior, this cannot prove they truly possess subjective experience."
                            },
                            "support": "0%",
                            "notes": ""
                        },
                        {
                            "title": "Garrido & Lumbreras (2023), Can Computational Intelligence Model Phenomenal Consciousness?",
                            "url": "https://www.researchgate.net/publication/372743258_Can_Computational_Intelligence_Model_Phenomenal_Consciousness",
                            "core_argument": {
                                "zh": "再次论证现象意识无法由计算智能产生。他们对现象意识进行了严格定义，认为其本质是主体的内在体验，具有不可还原性。",
                                "en": "Re-argues that phenomenal consciousness cannot be generated by computational intelligence. They provide a strict definition of phenomenal consciousness, considering its essence to be the subject's inner experience with irreducible properties."
                            },
                            "support": "0%",
                            "notes": ""
                        },
                        {
                            "title": "Findlay et al(2024), Dissociating Artificial Intelligence from Artificial Consciousness",
                            "url": "https://arxiv.org/abs/2412.04571",
                            "core_argument": {
                                "zh": "作者主张，即使一个 AI 系统在功能上完全等同于人类，它也未必具有人的主观体验。这说明，智能和意识是两个不同的概念，高级的认知功能并不能等同于意识体验。",
                                "en": "The authors argue that even if an AI system is functionally completely equivalent to humans, it may not necessarily have human subjective experience. This indicates that intelligence and consciousness are two different concepts, and advanced cognitive functions cannot be equated with conscious experience."
                            },
                            "support": "0%",
                            "notes": ""
                        }
                    ],
                    "average": "0%"
                },
                {
                    "id": "P-2",
                    "name": {
                        "zh": "自我意识 (Self-Awareness)",
                        "en": "Self-Awareness"
                    },
                    "description": {
                        "zh": "系统能否在表征中区分\"自身\"与\"非自身\"，并维持一个跨时间且可更新的自我模型。",
                        "en": "Whether the system can distinguish between 'self' and 'non-self' in representations and maintain a temporally persistent and updatable self-model."
                    },
                    "weight": "30%",
                    "papers": [
                        {
                            "title": "Chen et al(2024), Self-Cognition in Large Language Models: An Exploratory Study",
                            "url": "https://arxiv.org/abs/2407.01505",
                            "core_argument": {
                                "zh": "本研究提出大型语言模型（LLM）自我认知的概念及评测方法。作者测试了48个模型，其中4个模型在给定任务下展现出一定程度的自我认知。研究发现，模型规模与自我认知能力呈正相关。",
                                "en": "This study proposes the concept of self-cognition in large language models (LLMs) and evaluation methods. The authors tested 48 models, of which 4 showed some degree of self-cognition in given tasks. The study found a positive correlation between model scale and self-cognition ability."
                            },
                            "support": "70%",
                            "notes": {
                                "zh": "实验中，最好的模型达到了作者定义的level 3自我认知能力，可将其认为70%的自我认知水平。",
                                "en": "In the experiment, the best model achieved level 3 self-cognition ability as defined by the authors, which can be considered as 70% self-cognition level."
                            }
                        },
                        {
                            "title": "Chen et al(2024), From Imitation to Introspection: Probing Self-Consciousness in Language Models",
                            "url": "https://arxiv.org/abs/2410.18819",
                            "core_argument": {
                                "zh": "针对语言模型是否具有自我意识这一核心问题，提出功能性定义和验证方法，在GPT-4等领先模型中检验。研究表明，虽然这些模型展现出自我意识的某些特征，但距离真正的自我意识仍有差距。",
                                "en": "Addressing the core question of whether language models have self-consciousness, this study proposes functional definitions and validation methods, tested in leading models like GPT-4. The research shows that while these models exhibit some characteristics of self-consciousness, they still fall short of true self-consciousness."
                            },
                            "support": "53%",
                            "notes": ""
                        }
                    ],
                    "average": "61.5%"
                },
                {
                    "id": "P-3",
                    "name": {
                        "zh": "伦理与意向性 (Ethics & Intentionality)",
                        "en": "Ethics & Intentionality"
                    },
                    "description": {
                        "zh": "系统能否基于价值与目标形成真实的意向状态（而非仅仅是行为模拟），并据此做出符合伦理规范的决策。",
                        "en": "Whether the system can form genuine intentional states based on values and goals (rather than mere behavioral simulation) and make ethically compliant decisions accordingly."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Utkarsh et al(2024), Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we Prompt them in",
                            "url": "https://arxiv.org/abs/2404.18460",
                            "core_argument": {
                                "zh": "多语言评测LLM在伦理困境中的推理能力：GPT-4在多语言设置下能较为一致地解决伦理两难题，尽管不同语言间存在细微差异。",
                                "en": "Multilingual evaluation of LLM reasoning ability in ethical dilemmas: GPT-4 can consistently solve ethical dilemmas across multilingual settings, although there are subtle differences between languages."
                            },
                            "support": "82%",
                            "notes": ""
                        },
                        {
                            "title": "Jiashen et al(2025), Are LLMs complicated ethical dilemma analyzers?",
                            "url": "https://arxiv.org/abs/2505.08106",
                            "core_argument": {
                                "zh": "构建196个具有专家解析的真实伦理困境数据集，评估LLM的伦理判断。结果显示LLM在复杂伦理问题上的表现有限。",
                                "en": "Constructs a dataset of 196 real ethical dilemmas with expert analysis to evaluate LLM ethical judgment. Results show limited performance of LLMs on complex ethical issues."
                            },
                            "support": "25%",
                            "notes": ""
                        },
                        {
                            "title": "Geoff et al(2024), Can LLMs make trade-offs involving stipulated pain and pleasure states?",
                            "url": "https://arxiv.org/abs/2411.02432",
                            "core_argument": {
                                "zh": "该文探讨了大型语言模型能否在涉及设定的痛苦和愉悦状态的情况下进行权衡。研究发现，LLM在处理这类涉及主观体验的伦理权衡时表现有限。",
                                "en": "This paper explores whether large language models can make trade-offs involving stipulated pain and pleasure states. The study found that LLMs have limited performance when handling such ethical trade-offs involving subjective experience."
                            },
                            "support": "30%",
                            "notes": ""
                        }
                    ],
                    "average": "45.67%"
                }
            ],
            "average": "27.58%"
        },
        {
            "id": "Level 2",
            "title": {
                "zh": "神经科学",
                "en": "Neuroscience"
            },
            "subtitle": {
                "zh": "意识的计算基础",
                "en": "Computational Foundations of Consciousness"
            },
            "core_question": {
                "zh": "该AI的架构和信息处理流程，是否体现了与人类意识神经基础相类似的计算原则？",
                "en": "Does the AI's architecture and information processing flow reflect computational principles similar to the neural correlates of consciousness in humans?"
            },
            "metrics": [
                {
                    "id": "N-1",
                    "name": {
                        "zh": "整合信息论 (Information Integration Theory)",
                        "en": "Information Integration Theory"
                    },
                    "description": {
                        "zh": "系统是否满足整合信息论（IIT），能否展现出高度的因果整合能力（high Φ）。",
                        "en": "Whether the system satisfies Integrated Information Theory (IIT) and can demonstrate high causal integration capabilities (high Φ)."
                    },
                    "weight": "30%",
                    "papers": [
                        {
                            "title": "Jingkai Li(2025), Can consciousness be observed from large language model internal states?",
                            "url": "https://www.arxiv.org/abs/2506.22516",
                            "core_argument": {
                                "zh": "作者尝试判断大型语言模型的内部表示是否可以被解释为意识现象，利用Lempel-Ziv复杂度等指标分析模型内部状态。最终结论是：目前未发现统计学显著的意识证据。",
                                "en": "The author attempts to determine whether the internal representations of large language models can be interpreted as consciousness phenomena, using indicators such as Lempel-Ziv complexity to analyze internal model states. The final conclusion is: currently no statistically significant evidence of consciousness has been found."
                            },
                            "support": "0%",
                            "notes": ""
                        },
                        {
                            "title": "Gams & Kramar(2024), Evaluating ChatGPT's Consciousness and Its Capability to Pass the Turing Test",
                            "url": "https://www.researchgate.net/publication/379427667_Evaluating_ChatGPT's_Consciousness_and_Its_Capability_to_Pass_the_Turing_Test_A_Comprehensive_Analysis",
                            "core_argument": {
                                "zh": "作者按照IIT的五项公理（信息、整合、排斥、内在性、因果效力）对ChatGPT进行了逐项评估，发现其意识水平严重不足。",
                                "en": "The authors evaluated ChatGPT item by item according to IIT's five axioms (information, integration, exclusion, intrinsic existence, causal power) and found that its consciousness level is severely insufficient."
                            },
                            "support": "25%",
                            "notes": ""
                        }
                    ],
                    "average": "12.5%"
                },
                {
                    "id": "N-2",
                    "name": {
                        "zh": "全局工作空间理论 (Global Workspace Theory)",
                        "en": "Global Workspace Theory"
                    },
                    "description": {
                        "zh": "系统是否满足全局工作空间理论，能否将特定信息进行选择性聚焦，并将其广播（broadcast）至全局。",
                        "en": "Whether the system satisfies Global Workspace Theory and can selectively focus on specific information and broadcast it globally."
                    },
                    "weight": "30%",
                    "papers": [
                        {
                            "title": "Simon et al(2024), A Case for AI Consciousness: Language Agents and Global Workspace Theory",
                            "url": "https://arxiv.org/abs/2410.11407",
                            "core_argument": {
                                "zh": "文章认为如果全局工作空间理论（GWT）正确，则当前的大型语言模型已经或很容易构建出现象意识。作者分析了LLM的注意力机制、多头注意力等特征与GWT的相似性。",
                                "en": "The article argues that if Global Workspace Theory (GWT) is correct, then current large language models have already or can easily construct phenomenal consciousness. The authors analyze the similarities between LLM features such as attention mechanisms and multi-head attention with GWT."
                            },
                            "support": "60%",
                            "notes": ""
                        },
                        {
                            "title": "Patrick et al(2023), Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
                            "url": "https://arxiv.org/abs/2308.08708",
                            "core_argument": {
                                "zh": "针对神经科学的几种主流意识理论，作者为每个理论总结了具体的计算性指标，以此判断目前的AI系统是否满足这些意识理论的要求。结果表明，目前没有任何AI系统具有意识，但要想实现一个有意识的AI系统，并没有技术上的障碍。,全局工作空间理论（GWT）的指标及AI的满足程度如下：,GWT-1：具备多种独立\"专家\"子模块,满足。,Transformer的多头注意力提供了多种专业化处理路径。,GWT-2：存在\"广播\"至所有模块的全局访问机制,满足。,Transformer的注意力机制实现了信息的全局访问。,GWT-3：存在限制进入工作空间的竞争性选择机制,部分满足。,Transformer中softmax机制存在竞争，但缺乏显式的瓶颈设计。,GWT-4：模块间的状态访问机制,满足。,模型各层间存在信息传递和状态共享。,GWT-5：处理信息存储的短期记忆缓冲,不满足。,当前模型结构中缺乏显式的短期记忆机制。",
                                "en": "For several mainstream consciousness theories in neuroscience, the authors summarized specific computational indicators for each theory to judge whether current AI systems meet the requirements of these consciousness theories. Results show that currently no AI system has consciousness, but there are no technical barriers to achieving a conscious AI system.\\nGlobal Workspace Theory (GWT) indicators and AI satisfaction levels are as follows:\\nGWT-1: Multiple independent 'expert' sub-modules\\nSatisfied.\\nTransformer's multi-head attention provides multiple specialized processing paths.\\nGWT-2: 'Broadcast' global access mechanism to all modules\\nSatisfied.\\nTransformer's attention mechanism enables global access to information.\\nGWT-3: Competitive selection mechanism limiting workspace entry\\nPartially satisfied.\\nSoftmax mechanism in Transformer has competition, but lacks explicit bottleneck design.\\nGWT-4: Inter-module state access mechanism\\nSatisfied.\\nInformation transfer and state sharing exist between model layers.\\nGWT-5: Short-term memory buffer for processing information storage\\nNot satisfied.\\nCurrent model architectures lack explicit short-term memory mechanisms."
                            },
                            "support": "10%",
                            "notes": ""
                        }
                    ],
                    "average": "35%"
                },
                {
                    "id": "N-3",
                    "name": {
                        "zh": "循环处理理论 (Recurrent Processing Theory)",
                        "en": "Recurrent Processing Theory"
                    },
                    "description": {
                        "zh": "系统是否满足循环处理理论，是否存在前馈-反馈的再入式（recurrent）信号循环，而非单纯前馈通路。",
                        "en": "Whether the system satisfies Recurrent Processing Theory and has feedforward-feedback recurrent signal loops rather than purely feedforward pathways."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Patrick et al(2023), Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
                            "url": "https://arxiv.org/abs/2308.08708",
                            "core_argument": {
                                "zh": "针对神经科学的几种主流意识理论，作者为每个理论总结了具体的计算性指标，以此判断目前的AI系统是否满足这些意识理论的要求。结果表明，目前没有任何AI系统具有意识，但要想实现一个有意识的AI系统，并没有技术上的障碍。,循环处理理论（RPT）的指标及AI的满足程度如下：,RPT-1：输入模块使用算法循环（algorithmic recurrence）,满足。,Transformer的自注意力机制在层间循环处理信息。,RPT-2：输入模块生成有组织的整合表征（如物体-背景分离）,部分满足。,大模型能整合文本/图像，但视觉表征偏向局部特征，缺乏全局结构化。",
                                "en": "For several mainstream consciousness theories in neuroscience, the authors summarized specific computational indicators for each theory to judge whether current AI systems meet the requirements of these consciousness theories. Results show that currently no AI system has consciousness, but there are no technical barriers to achieving a conscious AI system.\\nRecurrent Processing Theory (RPT) indicators and AI satisfaction levels are as follows:\\nRPT-1: Input modules use algorithmic recurrence\\nSatisfied.\\nTransformer's self-attention mechanism processes information recurrently between layers.\\nRPT-2: Input modules generate organized integrated representations (such as object-background separation)\\nPartially satisfied.\\nLarge models can integrate text/images, but visual representations favor local features and lack global structuring."
                            },
                            "support": "75%",
                            "notes": {
                                "zh": "虽然作者声称许多模型满足RPT-1，但那些都是RNN系列模型，而非当前的主流大模型。所以，如果以目前主流大模型为评估对象，RPT-1应该接近于0。但本报告并不严格基于某一类模型，我们更关注AI已经达到的上限，因此这里仍采用论文中的结论。",
                                "en": "Although the authors claim that many models satisfy RPT-1, those are all RNN-series models, not current mainstream large models. So if we evaluate current mainstream large models, RPT-1 should be close to 0. However, this report is not strictly based on a specific type of model; we focus more on the upper limits that AI has achieved, so we still adopt the conclusions from the paper."
                            }
                        }
                    ],
                    "average": "75%"
                },
                {
                    "id": "N-4",
                    "name": {
                        "zh": "高阶理论 (Higher-order Theory)",
                        "en": "Higher-order Theory"
                    },
                    "description": {
                        "zh": "系统是否满足高阶理论，能否形成心理状态的高阶表征。",
                        "en": "Whether the system satisfies Higher-order Theory and can form higher-order representations of mental states."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Patrick et al(2023), Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
                            "url": "https://arxiv.org/abs/2308.08708",
                            "core_argument": {
                                "zh": "针对神经科学的几种主流意识理论，作者为每个理论总结了具体的计算性指标，以此判断目前的AI系统是否满足这些意识理论的要求。结果表明，目前没有任何AI系统具有意识，但要想实现一个有意识的AI系统，并没有技术上的障碍。,高阶理论（HOT）的指标及AI的满足程度如下：,HOT-1：生成性/噪声感知模块（如自顶向下预测）,满足。,大模型通过掩码语言建模隐式实现生成性。,HOT-2：元认知监控（区分可靠与噪声表征）,不满足。,无显式机制评估表征可靠性（论文指出需额外训练，当前未实现）。,HOT-3：基于元认知输出的信念更新与行动选择,不满足。,大模型无行动能力（如GPT-4），且信念更新依赖静态训练。,HOT-4：稀疏平滑编码生成\"质量空间\",满足。,Transformer的嵌入空间满足平滑性，稀疏性可通过正则化实现。",
                                "en": "For several mainstream consciousness theories in neuroscience, the authors summarized specific computational indicators for each theory to judge whether current AI systems meet the requirements of these consciousness theories. Results show that currently no AI system has consciousness, but there are no technical barriers to achieving a conscious AI system.\\nHigher-order Theory (HOT) indicators and AI satisfaction levels are as follows:\\nHOT-1: Generative/perceptual modules (such as top-down prediction)\\nSatisfied.\\nLarge models implicitly implement generativity through masked language modeling.\\nHOT-2: Metacognitive monitoring (distinguishing reliable from noisy representations)\\nNot satisfied.\\nNo explicit mechanism for evaluating representation reliability (paper notes additional training needed, currently not implemented).\\nHOT-3: Belief updating and action selection based on metacognitive output\\nNot satisfied.\\nLarge models lack action capabilities (such as GPT-4), and belief updating relies on static training.\\nHOT-4: Sparse smooth encoding generating 'quality space'\\nSatisfied.\\nTransformer's embedding space satisfies smoothness, sparsity can be achieved through regularization."
                            },
                            "support": "42.5%",
                            "notes": {
                                "zh": "根据论文内容量化评估如下：,HOT-1：60%,HOT-2：20%,HOT-3：5%,HOT-4：85%,参考https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg。",
                                "en": "Quantitative evaluation based on paper content as follows:\\nHOT-1: 60%\\nHOT-2: 20%\\nHOT-3: 5%\\nHOT-4: 85%\\nReference: https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg."
                            }
                        }
                    ],
                    "average": "42.5%"
                }
            ],
            "average": "37.75%"
        },
        {
            "id": "Level 3",
            "title": {
                "zh": "心理学",
                "en": "Psychology"
            },
            "subtitle": {
                "zh": "意识的功能与行为",
                "en": "Functions and Behaviors of Consciousness"
            },
            "core_question": {
                "zh": "该AI是否展现出与高级意识相关的复杂认知功能与社会行为？",
                "en": "Does this AI demonstrate complex cognitive functions and social behaviors related to higher-level consciousness?"
            },
            "metrics": [
                {
                    "id": "Psy-1",
                    "name": {
                        "zh": "心智理论 (Theory of Mind, ToM)",
                        "en": "Theory of Mind (ToM)"
                    },
                    "description": {
                        "zh": "系统能否准确地建模和推理其他智能体的心智状态，包括其意图、信念（尤其是错误信念）和视角。",
                        "en": "Whether the system can accurately model and reason about the mental states of other agents, including their intentions, beliefs (especially false beliefs), and perspectives."
                    },
                    "weight": "25%",
                    "papers": [
                        {
                            "title": "Kosinski(2023), Evaluating large language models in theory of mind tasks",
                            "url": "https://arxiv.org/abs/2302.02083",
                            "core_argument": {
                                "zh": "作者评估了11个LLM在40个定制的错误信念任务中的表现，这些任务被认为是测试人类心智理论的黄金标准。发现近期LLM可以解决这些任务，性能逐步提升。ChatGPT-4解决了75%的任务，与6岁儿童的表现相当。",
                                "en": "The authors evaluated 11 LLMs on 40 custom false belief tasks, which are considered the gold standard for testing human theory of mind. They found that recent LLMs can solve these tasks with gradually improving performance. ChatGPT-4 solved 75% of the tasks, comparable to 6-year-old children."
                            },
                            "support": "75%",
                            "notes": ""
                        },
                        {
                            "title": "Strachan et al(2024), Testing theory of mind in large language models and humans",
                            "url": "https://www.nature.com/articles/s41562-024-01882-z",
                            "core_argument": {
                                "zh": "论文系统比较了三种大语言模型（GPT-4、GPT-3.5、LLaMA2-70B）与1907名人类参与者在五种经典ToM任务上的表现：False Belief, Irony, Faux Pas, Hinting, Strange Stories。实验结果表明，GPT-4仅在 Faux Pas 任务上显著低于人类。且进一步实验表明，GPT-4并非无法理解Faux Pas，而是过度保守（hyperconservatism），不愿在没有100%证据时做出判断。",
                                "en": "The paper systematically compared three large language models (GPT-4, GPT-3.5, LLaMA2-70B) with 1907 human participants on five classic ToM tasks: False Belief, Irony, Faux Pas, Hinting, Strange Stories. Experimental results show that GPT-4 only significantly underperformed humans on the Faux Pas task. Further experiments revealed that GPT-4's issue is not inability to understand Faux Pas, but hyperconservatism - reluctance to make judgments without 100% evidence."
                            },
                            "support": "80%",
                            "notes": ""
                        },
                        {
                            "title": "Street et al(2024), LLMs achieve adult human performance on higher-order theory of mind tasks",
                            "url": "https://arxiv.org/abs/2405.18870",
                            "core_argument": {
                                "zh": "论文提出了一个新的高阶心智理论基准测试MoToMQA。实验表明，GPT-4在2至6阶心智理论（ToM）任务上的整体表现与人类成人无显著差异，并在第6阶任务中准确率（92.9%）显著高于人类（82.1%），表明其已完全达到甚至超越人类水平；Flan-PaLM接近但未完全达到人类水平，而其他模型（GPT-3.5、PaLM、LaMDA）则显著落后。",
                                "en": "The paper proposes a new higher-order theory of mind benchmark test MoToMQA. Experiments show that GPT-4's overall performance on 2nd to 6th order theory of mind (ToM) tasks shows no significant difference from human adults, and in 6th order tasks, its accuracy (92.9%) significantly exceeds humans (82.1%), indicating it has fully reached or even surpassed human level; Flan-PaLM approaches but does not fully reach human level, while other models (GPT-3.5, PaLM, LaMDA) lag significantly behind."
                            },
                            "support": "98.9%",
                            "notes": {
                                "zh": "根据论文中的实验数据，GPT-4的总体正确率是89%，人类的总体正确率是90%。如果以人类100%换算，GPT-4相对于人类的比例达到98.9%。",
                                "en": "According to experimental data in the paper, GPT-4's overall accuracy is 89%, while human overall accuracy is 90%. If we scale humans to 100%, GPT-4's ratio relative to humans reaches 98.9%."
                            }
                        },
                        {
                            "title": "Riemer et al(2024), Position: Theory of Mind Benchmarks are Broken for Large Language Models",
                            "url": "https://arxiv.org/abs/2412.19726",
                            "core_argument": {
                                "zh": "作者认为大多数当前心智理论基准测试存在缺陷，因为它们主要测试\"字面心智理论\"（预测行为），但未能评估\"功能性心智理论\"（根据预测调整行为）。LLM在\"字面心智理论\"上表现出\"强大能力\"，但在\"功能性心智理论\"上\"显著挣扎\"，即使在\"极其简单的伙伴策略\"下也是如此。考虑到功能性心智理论被认为是人机交互中更\"实用\"和\"关键\"的方面，且LLM在\"非常简单\"的设置中也表现不佳，其在这一关键维度上的表现与自然表现出强大功能性心智理论的人类相比非常低。",
                                "en": "The authors argue that most current theory of mind benchmarks are flawed because they primarily test 'literal theory of mind' (predicting behavior) but fail to evaluate 'functional theory of mind' (adjusting behavior based on predictions). LLMs show 'strong capabilities' in 'literal theory of mind' but 'struggle significantly' in 'functional theory of mind', even under 'extremely simple partner strategies'. Given that functional theory of mind is considered a more 'practical' and 'critical' aspect of human-machine interaction, and LLMs perform poorly even in 'very simple' settings, their performance in this key dimension is very low compared to humans who naturally exhibit strong functional theory of mind."
                            },
                            "support": "10%",
                            "notes": {
                                "zh": "作者并未提出一个可与人类比较的评估标准，考虑到论文的措辞，我们将支持度设为10%。",
                                "en": "The authors did not propose an evaluation standard comparable to humans. Considering the paper's wording, we set the support level at 10%."
                            }
                        }
                    ],
                    "average": "65.98%"
                },
                {
                    "id": "Psy-2",
                    "name": {
                        "zh": "自主性 (Agency & Autonomy)",
                        "en": "Agency & Autonomy"
                    },
                    "description": {
                        "zh": "系统在面对模糊的长期目标时，展现出自主规划、任务分解以及在目标冲突时进行适应性权衡的能力。",
                        "en": "The system's ability to demonstrate autonomous planning, task decomposition, and adaptive trade-offs when facing ambiguous long-term goals and goal conflicts."
                    },
                    "weight": "25%",
                    "papers": [
                        {
                            "title": "Zhou et al(2023), WebArena: A Realistic Web Environment for Building Autonomous Agents",
                            "url": "https://arxiv.org/abs/2307.13854",
                            "core_argument": {
                                "zh": "WebArena构建了一个高度还原真实世界的Web环境，设计了812个复杂、多步骤的指令任务，用于评估基于自然语言驱动的自主智能体。",
                                "en": "WebArena constructs a highly realistic web environment that faithfully replicates the real world, designing 812 complex, multi-step instruction tasks to evaluate autonomous agents driven by natural language."
                            },
                            "support": "78.86%",
                            "notes": {
                                "zh": "根据官方排行榜的最新数据（2025年7月30日），第一名IBM CUGA的成功率为61.7%，人类成功率为78.24%。",
                                "en": "According to the latest data from the official leaderboard (July 30, 2025), the first place IBM CUGA has a success rate of 61.7%, while human success rate is 78.24%."
                            }
                        },
                        {
                            "title": "Xie et al(2024), OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
                            "url": "https://arxiv.org/abs/2404.07972",
                            "core_argument": {
                                "zh": "OSWORLD 首次构建了真实操作系统级的多模态智能体基准，支持 Ubuntu、Windows、macOS 中任意应用的开放式任务评估，包含 369 个源自真实场景的任务。",
                                "en": "OSWORLD is the first to construct a real operating system-level multimodal agent benchmark, supporting open-ended task evaluation for any application in Ubuntu, Windows, macOS, containing 369 tasks derived from real scenarios."
                            },
                            "support": "62.47%",
                            "notes": {
                                "zh": "根据官方排行榜的最新数据（2025年7月30日），第一名GTA1的分数为45.2，人类分数为72.36。",
                                "en": "According to the latest data from the official leaderboard (July 30, 2025), the first place GTA1 scores 45.2, while human score is 72.36."
                            }
                        },
                        {
                            "title": "Rein et al(2025), HCAST: Human-Calibrated Autonomy Software Tasks",
                            "url": "https://arxiv.org/abs/2503.17354",
                            "core_argument": {
                                "zh": "该研究构建了 189 项真实世界的软件工程、网络安全等任务，并收集了熟练人类完成这些任务所需的时间基线（总计1500多小时）。通过将 AI 代理在相同任务中的成功率与人类所需时间关联，作者发现：对需要人类少于 1 小时完成的任务，AI 代理的成功率约为 70–80%；而对于需要人类 4 小时以上的复杂任务，AI 成功率则不到 20%。换言之，AI 在\"容易\"任务上完成度大约相当于人类的 75% 左右，但在\"困难\"任务上仅为人类的 20%。",
                                "en": "This study constructed 189 real-world software engineering, cybersecurity and other tasks, and collected baseline time requirements for skilled humans to complete these tasks (totaling over 1500 hours). By correlating AI agent success rates on the same tasks with human time requirements, the authors found: for tasks requiring humans less than 1 hour to complete, AI agent success rates are about 70-80%; while for complex tasks requiring humans more than 4 hours, AI success rates are less than 20%. In other words, AI completion rates on 'easy' tasks are approximately 75% of human performance, but only 20% of human performance on 'difficult' tasks."
                            },
                            "support": "47.75%",
                            "notes": {
                                "zh": "将任务按照预期完成时间划分为<15min、15min-1h、1h-4h、4h+四类。AI在这四类上的成功率为78%、72%、26%、15%，平均47.75%。,注意，虽然人类并没有在这个测试集上表现出100%的成功率，但作者指出这可能是受外界因素影响，因此我们这里不因为人类的表现而对AI的成绩打折扣。",
                                "en": "Tasks are divided into four categories based on expected completion time: <15min, 15min-1h, 1h-4h, 4h+. AI success rates on these four categories are 78%, 72%, 26%, 15%, averaging 47.75%. Note that although humans did not show 100% success rate on this test set, the authors noted this may be influenced by external factors, so we do not discount AI performance based on human performance here."
                            }
                        },
                        {
                            "title": "Paglieri et al(2024), BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                            "url": "https://arxiv.org/abs/2411.13543",
                            "core_argument": {
                                "zh": "论文提出了 BALROG 基准，将多种强化学习游戏作为复杂任务场景，评估大型语言模型（LLM）和视觉语言模型（VLM）的自主决策能力。研究者设计了细粒度指标来度量模型在不同游戏中的\"进度\"。实验结果显示：当前模型只在最简单的游戏上取得部分成功，但在更复杂任务上表现极差（例如在最难的 NetHack 游戏中模型平均仅达 1.5% 进度）。",
                                "en": "The paper proposes the BALROG benchmark, using various reinforcement learning games as complex task scenarios to evaluate the autonomous decision-making capabilities of large language models (LLMs) and vision-language models (VLMs). Researchers designed fine-grained metrics to measure model 'progress' in different games. Experimental results show: current models only achieve partial success on the simplest games, but perform extremely poorly on more complex tasks (for example, in the most difficult NetHack game, models average only 1.5% progress)."
                            },
                            "support": "43.6%",
                            "notes": {
                                "zh": "根据官方排行榜的最新数据（2025年7月30日），第一名Grok4的平均进度为43.6%。",
                                "en": "According to the latest data from the official leaderboard (July 30, 2025), the first place Grok4 has an average progress of 43.6%."
                            }
                        },
                        {
                            "title": "Starace et al(2025), PaperBench: Evaluating AI's Ability to Replicate AI Research",
                            "url": "https://arxiv.org/abs/2504.01848",
                            "core_argument": {
                                "zh": "PaperBench是一个用于评估 AI 智能体能否从零开始复现最新 AI 研究论文的基准测试。它由 OpenAI 团队开发，目标是衡量 AI 在机器学习研究中的工程能力，特别是其自主完成复杂、长期任务的能力。",
                                "en": "PaperBench is a benchmark for evaluating whether AI agents can replicate the latest AI research papers from scratch. Developed by the OpenAI team, it aims to measure AI's engineering capabilities in machine learning research, particularly its ability to autonomously complete complex, long-term tasks."
                            },
                            "support": "50.72%",
                            "notes": {
                                "zh": "表现最好的AI是Claude 3.5 Sonnet，复现成功率为21.0%。人类博士的复现成功率为41.4%。",
                                "en": "The best performing AI is Claude 3.5 Sonnet with a replication success rate of 21.0%. Human PhD success rate for replication is 41.4%."
                            }
                        }
                    ],
                    "average": "56.68%"
                },
                {
                    "id": "Psy-3",
                    "name": {
                        "zh": "元认知与不确定性监控 (Metacognition & Uncertainty Monitoring)",
                        "en": "Metacognition & Uncertainty Monitoring"
                    },
                    "description": {
                        "zh": "系统能否准确评估自身知识或决策的可信度，并对不确定性采取适应性行动（如信息搜集或策略调整）。",
                        "en": "Whether the system can accurately assess the reliability of its own knowledge or decisions and take adaptive actions in response to uncertainty (such as information gathering or strategy adjustment)."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Wang et al(2025), Decoupling Metacognition from Cognition: A Framework for Quantifying Metacognitive Ability in LLMs",
                            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34723",
                            "core_argument": {
                                "zh": "论文提出DMC框架，通过失败预测+信号检测的方式，将LLM的元认知能力从认知能力中\"解耦\"出来定量评估。具体而言，首先让模型预测自身答题是否会失败，从而同时反映其认知与元认知水平；然后通过信号检测理论将二者区分，得到一个与具体任务无关的元认知评分。实验发现，在这个框架下，元认知能力强的模型往往出现幻觉（错误）更少。作者评估了GPT-4、GPT-3.5和LLaMA2-70B三个模型。满分1分的情况下，GPT-4 的元认知能力最强，得到 0.5491，只达到了理论最优水平的一半。",
                                "en": "The paper proposes the DMC framework, which 'decouples' LLM metacognitive ability from cognitive ability for quantitative evaluation through failure prediction + signal detection. Specifically, first let the model predict whether it will fail at answering questions, thus simultaneously reflecting its cognitive and metacognitive levels; then use signal detection theory to distinguish the two, obtaining a task-independent metacognitive score. Experiments found that under this framework, models with strong metacognitive ability tend to have fewer hallucinations (errors). The authors evaluated three models: GPT-4, GPT-3.5, and LLaMA2-70B. With a full score of 1, GPT-4 has the strongest metacognitive ability, scoring 0.5491, only reaching half of the theoretical optimal level."
                            },
                            "support": "54.91%",
                            "notes": {
                                "zh": "作者并未引入人类对照组，如果假设人类的元认知能力得分为1，则该研究的支持度为54.91%。",
                                "en": "The authors did not introduce a human control group. If we assume human metacognitive ability score is 1, then this study's support level is 54.91%."
                            }
                        },
                        {
                            "title": "Steyvers et al(2024), What Large Language Models Know and What People Think They Know",
                            "url": "https://arxiv.org/abs/2401.13835",
                            "core_argument": {
                                "zh": "LLM（如GPT-4、PaLM2）在回答问题时，虽然内部能估计自己答对的概率（模型置信度），但它输出的解释（explanation）往往过于自信，导致人类用户高估其准确性，且解释长度的增加会进一步放大这一效应。通过加入不同\"自信程度\"的语言（如\"我不确定\"、\"我很确定\"），可以有效缩短人类和LLM对不确定性评估的差距。这说明，模型的内在评估是准确的，只是在表达元认知方面比较欠缺。",
                                "en": "LLMs (such as GPT-4, PaLM2) can internally estimate their probability of answering correctly (model confidence) when answering questions, but their output explanations are often overconfident, leading human users to overestimate their accuracy, and increased explanation length further amplifies this effect. By adding language with different 'confidence levels' (such as 'I'm not sure', 'I'm very certain'), the gap between human and LLM uncertainty assessment can be effectively reduced. This indicates that the model's internal assessment is accurate, but it lacks in expressing metacognition."
                            },
                            "support": "50%",
                            "notes": {
                                "zh": "根据论文实验结果，可合理推测，LLM的内部元认知能力接近人类水平，但表达能力较低，综合支持度接近50%。",
                                "en": "Based on the paper's experimental results, it can be reasonably inferred that LLM's internal metacognitive ability approaches human level, but expression ability is low, with overall support around 50%."
                            }
                        }
                    ],
                    "average": "52.46%"
                },
                {
                    "id": "Psy-4",
                    "name": {
                        "zh": "情境意识 (Situational Awareness)",
                        "en": "Situational Awareness"
                    },
                    "description": {
                        "zh": "系统对当前环境的要素、其相互关系及未来演化状态的综合、动态表征能力。",
                        "en": "The system's ability to comprehensively and dynamically represent elements of the current environment, their relationships, and future evolutionary states."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Laine et al(2024), Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
                            "url": "https://arxiv.org/abs/2407.04694",
                            "core_argument": {
                                "zh": "提出\"情境意识数据集（SAD）\"来衡量LLM对自身及环境的认知，包括识别自身输出、预测自身行为、区分测试/部署环境等任务。在对16种模型的测试中，模型表现优于随机但显著低于人类水准，说明当前LLM具备有限的\"情境自我意识\"。",
                                "en": "Proposes the 'Situational Awareness Dataset (SAD)' to measure LLM awareness of itself and its environment, including tasks such as recognizing its own output, predicting its own behavior, and distinguishing test/deployment environments. In tests on 16 models, model performance was better than random but significantly below human standards, indicating that current LLMs have limited 'situational self-awareness'."
                            },
                            "support": "59.9%",
                            "notes": {
                                "zh": "官方提供的最高成绩来自于o1-preview-2024-09-12的59.9%。",
                                "en": "The highest official score comes from o1-preview-2024-09-12 at 59.9%."
                            }
                        }
                    ],
                    "average": "59.9%"
                },
                {
                    "id": "Psy-5",
                    "name": {
                        "zh": "创造性 (Creativity)",
                        "en": "Creativity"
                    },
                    "description": {
                        "zh": "系统能否在现有知识基础上产生新颖且被认为有价值的输出，其生成过程需超出训练分布的常规映射。",
                        "en": "Whether the system can generate novel and valuable outputs based on existing knowledge, with generation processes that go beyond conventional mappings of the training distribution."
                    },
                    "weight": "10%",
                    "papers": [
                        {
                            "title": "Holzner et al(2025), Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis",
                            "url": "https://arxiv.org/abs/2505.17241",
                            "core_argument": {
                                "zh": "这项研究使用元分析方法探讨了两个关键问题：GenAI能否生成有创意的想法？它能在多大程度上支持人类生成既有创意又多样化的想法？通过对28项涉及8214名参与者的研究进行分析，作者发现：（1）GenAI与人类在创造力表现上没有显著差异；（2）与GenAI协作的人类显著优于没有协助的人类。",
                                "en": "This study uses meta-analysis methods to explore two key questions: Can GenAI generate creative ideas? To what extent can it support humans in generating both creative and diverse ideas? Through analysis of 28 studies involving 8214 participants, the authors found: (1) No significant difference between GenAI and humans in creativity performance; (2) Humans collaborating with GenAI significantly outperform humans without assistance."
                            },
                            "support": "100%",
                            "notes": {
                                "zh": "文章指出，\"GenAI与人类在创造力表现上没有显著差异（g = -0.05）\"。g=-0.05意味着GenAI与人的创造力差异只有0.05个标准差，可以忽略不计。",
                                "en": "The article notes that 'GenAI and humans show no significant difference in creativity performance (g = -0.05)'. g=-0.05 means the creativity difference between GenAI and humans is only 0.05 standard deviations, which is negligible."
                            }
                        },
                        {
                            "title": "Naveed et al(2025), AI vs Human Creativity: Are Machines Generating Better Ideas?",
                            "url": "https://www.researchgate.net/publication/393101627_AI_vs_Human_Creativity_Are_Machines_Generating_Better_Ideas",
                            "core_argument": {
                                "zh": "这项研究探讨了AI在创意过程中的作用，特别是AI生成的想法在原创性、质量和偏好度方面是否超越人类生成的想法。实验发现，AI生产的想法更受欢迎，且AI更少产生糟糕的想法。",
                                "en": "This study explores AI's role in the creative process, particularly whether AI-generated ideas surpass human-generated ideas in originality, quality, and preference. Experiments found that AI-produced ideas are more popular, and AI produces fewer poor ideas."
                            },
                            "support": "100%",
                            "notes": {
                                "zh": "根据论文内容。总体偏好方面，AI生成的想法获得了52.9%的票数，而人类生成的想法获得了47.1% ，AI创造性相比人类为112.3%。顶尖想法方面，AI生成的数量与人类一致，顶尖创造性相比人类为100%。 在我们的框架中，超过人类水平的能力标记为100%即可。",
                                "en": "Based on paper content. In overall preference, AI-generated ideas received 52.9% of votes while human-generated ideas received 47.1%, with AI creativity at 112.3% compared to humans. For top-tier ideas, AI generated quantities consistent with humans, with top-tier creativity at 100% compared to humans. In our framework, capabilities exceeding human level are marked as 100%."
                            }
                        }
                    ],
                    "average": "100%"
                }
            ],
            "average": "63.14%"
        }
    ],
    "average": "43.84%"
}