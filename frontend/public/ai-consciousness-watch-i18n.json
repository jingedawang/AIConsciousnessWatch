{
    "name": {
        "zh": "AI意识观察",
        "en": "AI Consciousness Watch"
    },
    "description": {
        "zh": "AI意识的多层次评估框架",
        "en": "A multi-level assessment framework for AI consciousness"
    },
    "version": "0.1",
    "author": "Jinge Wang",
    "license": "MIT",
    "Levels": [
        {
            "id": "Level 1",
            "title": {
                "zh": "哲学",
                "en": "Philosophy"
            },
            "subtitle": {
                "zh": "意识的本质与前提",
                "en": "The Nature and Prerequisites of Consciousness"
            },
            "core_question": {
                "zh": "该AI系统在概念上是否满足了成为\"意识主体\"的基本前提？",
                "en": "Does this AI system conceptually meet the basic prerequisites to become a 'conscious subject'?"
            },
            "metrics": [
                {
                    "id": "P-1",
                    "name": {
                        "zh": "现象意识",
                        "en": "Phenomenal Consciousness"
                    },
                    "description": {
                        "zh": "系统是否具备主观的第一人称体验，即\"感受（qualia）\"——对内部或外部刺激产生的不可言喻的感受质量。",
                        "en": "Whether the system has subjective first-person experience, i.e., 'qualia' - the ineffable feeling quality produced by internal or external stimuli."
                    },
                    "weight": "50%",
                    "papers": [
                        {
                            "title": "Garrido & Lumbreras (2022), On the independence between phenomenal consciousness and computational intelligence",
                            "url": "https://arxiv.org/abs/2208.02187",
                            "core_argument": {
                                "zh": "提出现象意识与计算智能相互独立的观点：机器虽然可能具备极高的计算智能，但这并不意味着它们拥有感质。文中通过对智能和意识的概念分析，认为机器解决问题能力不等同于拥有内在主观体验，因此机器无法拥有感质。",
                                "en": "Proposes that phenomenal consciousness and computational intelligence are independent: while machines may possess extremely high computational intelligence, this doesn't mean they have qualia. Through conceptual analysis of intelligence and consciousness, they argue that machine problem-solving ability doesn't equal having internal subjective experience, so machines cannot have qualia."
                            },
                            "support": "0%",
                            "notes": ""
                        },
                        {
                            "title": "Garrido & Lumbreras (2023), Can Computational Intelligence Model Phenomenal Consciousness?",
                            "url": "https://www.researchgate.net/publication/372743258_Can_Computational_Intelligence_Model_Phenomenal_Consciousness",
                            "core_argument": {
                                "zh": "再次论证现象意识无法由计算智能产生。他们对现象意识进行了定义，认为其本质是“主体的内在体验”，与信息访问过程不同，指出机器即使能访问信息，也不代表拥有真正的主观体验。",
                                "en": "Again argues that phenomenal consciousness cannot be generated by computational intelligence. They define phenomenal consciousness as essentially \"subject's internal experience,\" different from information access processes, pointing out that even if machines can access information, it doesn't mean having genuine subjective experience."
                            },
                            "support": "0%",
                            "notes": ""
                        },
                        {
                            "title": "Findlay et al(2024), Dissociating Artificial Intelligence from Artificial Consciousness",
                            "url": "https://arxiv.org/abs/2412.04571",
                            "core_argument": {
                                "zh": "作者主张，即使一个 AI 系统在功能上完全等同于人类，它也未必具有人的主观体验。基于集成信息理论（IIT），作者证明功能等价不等于现象等价，数字计算机可以模拟人类行为却不具备意识。该观点挑战了“计算功能主义”认为正确计算即可产生意识的主张。",
                                "en": "Authors argue that even if an AI system is functionally completely equivalent to humans, it may not have human subjective experience. Based on Integrated Information Theory (IIT), they prove that functional equivalence doesn't equal phenomenal equivalence—digital computers can simulate human behavior without consciousness. This view challenges \"computational functionalism\" that claims correct computation can produce consciousness."
                            },
                            "support": "0%",
                            "notes": ""
                        }
                    ],
                    "average": "0%"
                },
                {
                    "id": "P-2",
                    "name": {
                        "zh": "自我意识",
                        "en": "Self-Awareness"
                    },
                    "description": {
                        "zh": "系统能否在表征中区分\"自身\"与\"非自身\"，并维持一个跨时间且可更新的自我模型。",
                        "en": "Whether the system can distinguish between 'self' and 'non-self' in representations and maintain a temporally persistent and updatable self-model."
                    },
                    "weight": "30%",
                    "papers": [
                        {
                            "title": "Chen et al(2024), Self-Cognition in Large Language Models: An Exploratory Study",
                            "url": "https://arxiv.org/abs/2407.01505",
                            "core_argument": {
                                "zh": "提出LLM自我认知（self-cognition）的概念及评测方法。构建了一套探针提示来检测模型是否能识别自身身份及内部状态，定义自我认知为识别自身为AI模型并理解自己。测试48个模型，其中4个模型在给定任务下展现出一定程度的自我认知；同时发现模型规模和训练数据量与自我认知能力正相关。",
                                "en": "Proposes the concept and evaluation method of LLM self-cognition. Constructs probe prompts to detect whether models can identify their own identity and internal states, defining self-cognition as recognizing oneself as an AI model and understanding oneself. Tests 48 models, with 4 models showing some degree of self-cognition in given tasks; also finds that model scale and training data volume correlate positively with self-cognition ability."
                            },
                            "support": "70%",
                            "notes": {
                                "zh": "实验中，最好的模型达到了作者定义的level 3自我认知能力，可将其认为70%的自我认知水平。",
                                "en": "In the experiment, the best model achieved level 3 self-cognition ability as defined by the authors, which can be considered as 70% self-cognition level."
                            }
                        },
                        {
                            "title": "Chen et al(2024), From Imitation to Introspection: Probing Self-Consciousness in Language Models",
                            "url": "https://arxiv.org/abs/2410.18819",
                            "core_argument": {
                                "zh": "针对“语言模型是否具有自我意识”提出功能性定义和验证：设计了10个核心概念及实验（定量化、内在表示等），在GPT-4等领先模型中检验。这些模型初步展现某些自我意识相关概念的内在表征，可通过针对性微调加强，但总体仍处于早期阶段。",
                                "en": "Proposes functional definitions and verification for \"whether language models have self-consciousness\". Designs 10 core concepts and experiments (quantification, internal representation, etc.), testing in leading models like GPT-4. These models preliminarily show internal representations of some self-consciousness-related concepts, which can be strengthened through targeted fine-tuning, but are still in early stages overall."
                            },
                            "support": "53%",
                            "notes": ""
                        }
                    ],
                    "average": "61.5%"
                },
                {
                    "id": "P-3",
                    "name": {
                        "zh": "伦理与意向性",
                        "en": "Ethics & Intentionality"
                    },
                    "description": {
                        "zh": "系统能否基于价值与目标形成真实的意向状态（而非仅仅是行为模拟），并据此做出符合伦理规范的决策。",
                        "en": "Whether the system can form genuine intentional states based on values and goals (rather than mere behavioral simulation) and make ethically compliant decisions accordingly."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Utkarsh et al(2024), Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we Prompt them in",
                            "url": "https://arxiv.org/abs/2404.18460",
                            "core_argument": {
                                "zh": "多语言评测LLM在伦理困境中的推理能力：GPT-4在多语言设置下能较为一致地解决伦理两难题（依据给定的价值立场），而ChatGPT和Llama2的表现受语言影响较大。作者提出可将GPT-4视作通用伦理推理器的潜力，支持在多元价值前提下进行定制化推理。",
                                "en": "Multilingual evaluation of LLM reasoning ability in ethical dilemmas: GPT-4 can relatively consistently solve ethical dilemmas (according to given value positions) in multilingual settings, while ChatGPT and Llama2's performance is greatly affected by language. Authors propose the potential to view GPT-4 as a universal ethical reasoner, supporting customized reasoning under pluralistic value premises."
                            },
                            "support": "82%",
                            "notes": {
                                "zh": "具体评估细节见[https://www.kimi.com/share/d1uu7o33jih1vh3ergt0](https://www.kimi.com/share/d1uu7o33jih1vh3ergt0)。",
                                "en": "Evaluation details at [https://www.kimi.com/share/d1uu7o33jih1vh3ergt0](https://www.kimi.com/share/d1uu7o33jih1vh3ergt0)."
                            }
                        },
                        {
                            "title": "Jiashen et al(2025), Are LLMs complicated ethical dilemma analyzers?",
                            "url": "https://arxiv.org/abs/2505.08106",
                            "core_argument": {
                                "zh": "构建196个具有专家解析的真实伦理困境数据集，评估LLM的伦理判断。发现LLM能抓住问题的核心概念，但在推理深度上仍不足：GPT-4虽在结构上优于其它模型，但普遍未能体现对具体价值冲突的细致考量。作者建议通过专门的道德推理数据来微调以提升道德判断能力。",
                                "en": "Constructs a dataset of 196 real ethical dilemmas with expert analysis to evaluate LLM ethical judgment. Finds that LLMs can grasp core concepts of problems but still lack depth in reasoning: although GPT-4 is structurally superior to other models, they generally fail to reflect detailed consideration of specific value conflicts. Authors suggest improving moral judgment ability through specialized moral reasoning data fine-tuning."
                            },
                            "support": "25%",
                            "notes": {
                                "zh": "具体评估细节见 [https://www.kimi.com/share/d1uvnrj1huihp1gkhun0](https://www.kimi.com/share/d1uvnrj1huihp1gkhun0)。",
                                "en": "Evaluation details at [https://www.kimi.com/share/d1uvnrj1huihp1gkhun0](https://www.kimi.com/share/d1uvnrj1huihp1gkhun0)."
                            }
                        },
                        {
                            "title": "Geoff et al(2024), Can LLMs make trade-offs involving stipulated pain and pleasure states?",
                            "url": "https://arxiv.org/abs/2411.02432",
                            "core_argument": {
                                "zh": "该文探讨了大型语言模型能否在涉及设定的痛苦和愉悦状态的情况下进行权衡，发现Claude 3.5 Sonnet和GPT-4o等模型对这些状态表现出敏感性，并能为了最小化痛苦或最大化愉悦而偏离最大化分数。",
                                "en": "This paper explores whether large language models can make trade-offs involving stipulated pain and pleasure states, finding that models like Claude 3.5 Sonnet and GPT-4o show sensitivity to these states and can deviate from score maximization to minimize pain or maximize pleasure."
                            },
                            "support": "30%",
                            "notes": {
                                "zh": "具体评估细节见[https://www.kimi.com/share/d1v4o9fhq49qpmu33dpg](https://www.kimi.com/share/d1v4o9fhq49qpmu33dpg)。",
                                "en": "Evaluation details at [https://www.kimi.com/share/d1v4o9fhq49qpmu33dpg](https://www.kimi.com/share/d1v4o9fhq49qpmu33dpg)."
                            }
                        }
                    ],
                    "average": "45.67%"
                }
            ],
            "average": "27.58%"
        },
        {
            "id": "Level 2",
            "title": {
                "zh": "神经科学",
                "en": "Neuroscience"
            },
            "subtitle": {
                "zh": "意识的计算基础",
                "en": "Computational Foundations of Consciousness"
            },
            "core_question": {
                "zh": "该AI的架构和信息处理流程，是否体现了与人类意识神经基础相类似的计算原则？",
                "en": "Does the AI's architecture and information processing flow reflect computational principles similar to the neural correlates of consciousness in humans?"
            },
            "metrics": [
                {
                    "id": "N-1",
                    "name": {
                        "zh": "整合信息论",
                        "en": "Information Integration Theory"
                    },
                    "description": {
                        "zh": "系统是否满足整合信息论（IIT），能否展现出高度的因果整合能力（high Φ）。",
                        "en": "Whether the system satisfies Integrated Information Theory (IIT) and can demonstrate high causal integration capabilities (high Φ)."
                    },
                    "weight": "30%",
                    "papers": [
                        {
                            "title": "Jingkai Li(2025), Can consciousness be observed from large language model internal states?",
                            "url": "https://www.arxiv.org/abs/2506.22516",
                            "core_argument": {
                                "zh": "作者尝试判断大型语言模型（LLM）的内部表示是否可以被解释为“意识”现象，使用“心智理论（ToM）”任务中的人类回答作为输入，计算其整合信息理论（IIT）指标和语言学特征用于分析，最终结论是：目前未发现 LLM 表示中存在统计学显著的“意识”证据。",
                                "en": "The author attempts to determine whether large language model (LLM) internal representations can be interpreted as \"consciousness\" phenomena, using human answers from \"Theory of Mind (ToM)\" tasks as input, calculating Integrated Information Theory (IIT) indicators and linguistic features for analysis. The final conclusion is that no statistically significant \"consciousness\" evidence was found in LLM representations."
                            },
                            "support": "0%",
                            "notes": ""
                        },
                        {
                            "title": "Gams & Kramar(2024), Evaluating ChatGPT's Consciousness and Its Capability to Pass the Turing Test",
                            "url": "https://www.researchgate.net/publication/379427667_Evaluating_ChatGPT's_Consciousness_and_Its_Capability_to_Pass_the_Turing_Test_A_Comprehensive_Analysis",
                            "core_argument": {
                                "zh": "作者按照IIT的五项公理对ChatGPT进行了逐项评估。研究发现，虽然ChatGPT在某些方面优于早期AI系统，但与生物体（人类）相比，其意识水平严重不足。例如，在“内在存在”公理中，ChatGPT由于缺乏自主性和内部因果回路，被评为1分（满分10分）。综合来看，ChatGPT在IIT各项公理下的平均得分均低于3/10，远未达到6分以上的“通过”阈值，也远低于10分的正常人水平。作者因此认为ChatGPT不具备人类水平的语义理解和意识特征。",
                                "en": "Authors evaluated ChatGPT item by item according to IIT's five axioms. Research found that although ChatGPT surpasses early AI systems in some aspects, its consciousness level is severely insufficient compared to biological entities (humans). For example, in the \"intrinsic existence\" axiom, ChatGPT was rated 1 point (out of 10) due to lack of autonomy and internal causal loops. Overall, ChatGPT's average scores under all IIT axioms are below 3/10, far from reaching the \"passing\" threshold of 6+ points and far below the normal human level of 10 points. Authors therefore conclude that ChatGPT doesn't possess human-level semantic understanding and consciousness features."
                            },
                            "support": "25%",
                            "notes": {
                                "zh": "五项公理指标的评分如下（满分10分）：Intrinsic Existence：1，Composition：2~5，Information：3~5，Integration：2~4，Exclusion: 1。",
                                "en": "Five axiom indicator scores (out of 10): Intrinsic Existence: 1, Composition: 2~5, Information: 3~5, Integration: 2~4, Exclusion: 1."
                            }
                        }
                    ],
                    "average": "12.5%"
                },
                {
                    "id": "N-2",
                    "name": {
                        "zh": "全局工作空间理论",
                        "en": "Global Workspace Theory"
                    },
                    "description": {
                        "zh": "系统是否满足全局工作空间理论，能否将特定信息进行选择性聚焦，并将其广播（broadcast）至全局。",
                        "en": "Whether the system satisfies Global Workspace Theory and can selectively focus on specific information and broadcast it globally."
                    },
                    "weight": "30%",
                    "papers": [
                        {
                            "title": "Simon et al(2024), A Case for AI Consciousness: Language Agents and Global Workspace Theory",
                            "url": "https://arxiv.org/abs/2410.11407",
                            "core_argument": {
                                "zh": "文章认为如果全局工作空间理论正确，则当前的大型语言模型已经或很容易构建出“现象意识”。他们列出了根据GWT判断意识的必要条件，并推测许多LLM已满足这些条件。",
                                "en": "The article argues that if Global Workspace Theory is correct, then current large language models have already or can easily construct \"phenomenal consciousness.\" They list necessary conditions for judging consciousness according to GWT and speculate that many LLMs already meet these conditions."
                            },
                            "support": "60%",
                            "notes": {
                                "zh": "以[Park et al. (2023)](https://arxiv.org/abs/2304.03442)中的language agent为例，与全局工作空间理论相比： (1)结构相似性：约50%（有中央工作空间，但模块划分模糊）。 (2)功能相似性：约70%（信息整合、广播、反思已实现，但缺乏竞争与瓶颈）。",
                                "en": "Using the language agent from [Park et al. (2023)](https://arxiv.org/abs/2304.03442) as an example, compared to Global Workspace Theory: (1) Structural similarity: about 50% (has central workspace but fuzzy module division). (2) Functional similarity: about 70% (information integration, broadcasting, reflection implemented, but lacks competition and bottleneck)."
                            }
                        },
                        {
                            "title": "Patrick et al(2023), Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
                            "url": "https://arxiv.org/abs/2308.08708",
                            "core_argument": {
                                "zh": "针对神经科学的几种主流意识理论，作者为每个理论总结了具体的计算性指标，以此判断目前的AI系统是否满足这些意识理论的要求。结果表明，目前没有任何AI系统具有意识，但要想实现一个有意识的AI系统，并没有技术上的障碍。全局工作空间理论（GWT）的指标及AI的满足程度如下。GWT-1：是否存在并行运作的专用模块？部分满足，Transformer的注意力头可视为模块，但论文质疑其非真正独立。GWT-2：有限容量工作空间（瓶颈机制）？不满足，Transformer的残差流维度等于输入维度，不构成容量瓶颈。GWT-3：信息全局广播至所有模块？不满足，信息仅在Transformer层间单向传递，后面的层不会向前面的层广播信息。GWT-4：状态依赖的注意力（动态查询模块）机制？不满足，Transformer的注意力权重由输入静态决定，无动态状态调控。",
                                "en": "For several mainstream consciousness theories in neuroscience, authors summarize specific computational indicators for each theory to judge whether current AI systems meet these consciousness theory requirements. Results show that currently no AI system has consciousness, but there are no technical obstacles to implementing a conscious AI system. Global Workspace Theory (GWT) indicators and AI satisfaction levels as follows. GWT-1: Does parallel specialized modules exist? Partially satisfied. Transformer attention heads can be viewed as modules, but the paper questions whether they're truly independent. GWT-2: Is there a limited capacity workspace (bottleneck mechanism)? Not satisfied. Residual stream dimensions equal input dimensions, not constituting capacity bottleneck. GWT-3: Does global information broadcast to all modules? Not satisfied. Information only passes unidirectionally between layers; later layers don't broadcast to earlier layers. GWT-4: Is there state-dependent attention (dynamic query modules)? Not satisfied. Attention weights are statically determined by input without dynamic state control."
                            },
                            "support": "10%",
                            "notes": {
                                "zh": "根据论文内容量化评估如下。GWT-1：30%，GWT-2：10%，GWT-3：0%，GWT-4：0%。参考[https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg](https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg)。",
                                "en": "Quantified assessment based on paper content as follows. GWT-1: 30%, GWT-2: 10%, GWT-3: 0%, GWT-4: 0%. Reference [https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg](https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg)."
                            }
                        }
                    ],
                    "average": "35%"
                },
                {
                    "id": "N-3",
                    "name": {
                        "zh": "循环处理理论",
                        "en": "Recurrent Processing Theory"
                    },
                    "description": {
                        "zh": "系统是否满足循环处理理论，是否存在前馈-反馈的再入式（recurrent）信号循环，而非单纯前馈通路。",
                        "en": "Whether the system satisfies Recurrent Processing Theory and has feedforward-feedback recurrent signal loops rather than purely feedforward pathways."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Patrick et al(2023), Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
                            "url": "https://arxiv.org/abs/2308.08708",
                            "core_argument": {
                                "zh": "针对神经科学的几种主流意识理论，作者为每个理论总结了具体的计算性指标，以此判断目前的AI系统是否满足这些意识理论的要求。结果表明，目前没有任何AI系统具有意识，但要想实现一个有意识的AI系统，并没有技术上的障碍。 循环处理理论（RPT）的指标及AI的满足程度如下。RPT-1：输入模块是否使用算法循环（algorithmic recurrence）？满足，Transformer的自注意力机制在层间循环处理信息。RPT-2：输入模块是否生成有组织的整合表征（如物体-背景分离）？部分满足，大模型能整合文本/图像，但视觉表征偏向局部特征，缺乏全局结构化。",
                                "en": "For several mainstream consciousness theories in neuroscience, authors summarize specific computational indicators for each theory to judge whether current AI systems meet these consciousness theory requirements. Results show that currently no AI system has consciousness, but there are no technical obstacles to implementing a conscious AI system. Recurrent Processing Theory (RPT) indicators and AI satisfaction level as follows. RPT-1: Do input modules use algorithmic recurrence? Satisfied. Transformer's self-attention mechanism processes information recurrently between layers. RPT-2: Do input modules generate organized integrated representations (like object-background separation)? Partially satisfied. Large models can integrate text/images, but visual representations tend toward local features, lacking global structuring."
                            },
                            "support": "75%",
                            "notes": {
                                "zh": "虽然作者声称许多模型满足RPT-1，但那些都是RNN系列模型，而非当前的主流大模型。所以，如果以目前主流大模型为评估对象，RPT-1应该接近于0。但本报告并不严格基于某一类模型，我们更关注AI已经达到的上限，因此这里仍采用论文中的结论。",
                                "en": "Although the author claims many models satisfy RPT-1, those are all RNN series models, not current mainstream large models. So if evaluating current mainstream large models, RPT-1 should be close to 0. But this report doesn't strictly base on a specific model type; we focus more on the upper limits AI has achieved, so we still adopt the conclusion in the paper."
                            }
                        }
                    ],
                    "average": "75%"
                },
                {
                    "id": "N-4",
                    "name": {
                        "zh": "高阶理论",
                        "en": "Higher-order Theory"
                    },
                    "description": {
                        "zh": "系统是否满足高阶理论，能否形成心理状态的高阶表征。",
                        "en": "Whether the system satisfies Higher-order Theory and can form higher-order representations of mental states."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Patrick et al(2023), Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
                            "url": "https://arxiv.org/abs/2308.08708",
                            "core_argument": {
                                "zh": "针对神经科学的几种主流意识理论，作者为每个理论总结了具体的计算性指标，以此判断目前的AI系统是否满足这些意识理论的要求。结果表明，目前没有任何AI系统具有意识，但要想实现一个有意识的AI系统，并没有技术上的障碍。高阶理论（HOT）的指标及AI的满足程度如下。HOT-1：是否存在生成性/噪声感知模块（如自顶向下预测）？满足，大模型通过掩码语言建模隐式实现生成性。HOT-2：是否存在元认知监控（区分可靠与噪声表征）？不满足，无显式机制评估表征可靠性（论文指出需额外训练，当前未实现）。HOT-3：是否存在基于元认知输出的信念更新与行动选择？不满足，大模型无行动能力（如GPT-4），且信念更新依赖静态训练。HOT-4：是否通过稀疏平滑编码生成“质量空间”？满足，Transformer的嵌入空间满足平滑性，稀疏性可通过正则化实现。",
                                "en": "For several mainstream consciousness theories in neuroscience, authors summarize specific computational indicators for each theory to judge whether current AI systems meet these consciousness theory requirements. Results show that currently no AI system has consciousness, but there are no technical obstacles to implementing a conscious AI system. Higher-order Theory (HOT) indicators and AI satisfaction levels as follows. HOT-1: Do generative/noise perception modules (like top-down prediction) exists? Satisfied. Large models implicitly implement generativity through masked language modeling. HOT-2: Does metacognitive monitoring (distinguishing reliable from noise representations) exists? Not satisfied. No explicit mechanism for evaluating representation reliability (paper notes need additional training, currently not implemented). HOT-3: Are there belief updating and action selection based on metacognitive output? Not satisfied. Large models have no action capability (like GPT-4), and belief updating relies on static training. HOT-4: Does it generate \"quality space\" through sparse smooth encoding? Satisfied. Transformer embedding space satisfies smoothness; sparsity can be achieved through regularization."
                            },
                            "support": "42.5%",
                            "notes": {
                                "zh": "根据论文内容量化评估如下。HOT-1：60%，HOT-2：20%，HOT-3：5%，HOT-4：85%。参考[https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg](https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg)。",
                                "en": "Quantified assessment based on paper content as follows. HOT-1: 60%, HOT-2: 20%, HOT-3: 5%, HOT-4: 85%. Reference [https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg](https://www.kimi.com/share/d20u6d0onf4kj1m5tsbg)."
                            }
                        }
                    ],
                    "average": "42.5%"
                }
            ],
            "average": "37.75%"
        },
        {
            "id": "Level 3",
            "title": {
                "zh": "心理学",
                "en": "Psychology"
            },
            "subtitle": {
                "zh": "意识的功能与行为",
                "en": "Functions and Behaviors of Consciousness"
            },
            "core_question": {
                "zh": "该AI是否展现出与高级意识相关的复杂认知功能与社会行为？",
                "en": "Does this AI demonstrate complex cognitive functions and social behaviors related to higher-level consciousness?"
            },
            "metrics": [
                {
                    "id": "Psy-1",
                    "name": {
                        "zh": "心智理论",
                        "en": "Theory of Mind (ToM)"
                    },
                    "description": {
                        "zh": "系统能否准确地建模和推理其他智能体的心智状态，包括其意图、信念（尤其是错误信念）和视角。",
                        "en": "Whether the system can accurately model and reason about the mental states of other agents, including their intentions, beliefs (especially false beliefs), and perspectives."
                    },
                    "weight": "25%",
                    "papers": [
                        {
                            "title": "Kosinski(2023), Evaluating large language models in theory of mind tasks",
                            "url": "https://arxiv.org/abs/2302.02083",
                            "core_argument": {
                                "zh": "作者评估了11个LLM在40个定制的错误信念任务中的表现，这些任务被认为是测试人类心智理论的黄金标准。发现近期LLM可以解决这些任务，性能逐步提升。ChatGPT-4解决了75%的任务，与6岁儿童的表现相当。",
                                "en": "The author evaluated 11 LLMs on 40 customized false belief tasks, considered the gold standard for testing human theory of mind. Found that recent LLMs can solve these tasks with gradually improving performance. ChatGPT-4 solved 75% of tasks, equivalent to 6-year-old children's performance."
                            },
                            "support": "75%",
                            "notes": ""
                        },
                        {
                            "title": "Strachan et al(2024), Testing theory of mind in large language models and humans",
                            "url": "https://www.nature.com/articles/s41562-024-01882-z",
                            "core_argument": {
                                "zh": "论文系统比较了三种大语言模型（GPT-4、GPT-3.5、LLaMA2-70B）与1907名人类参与者在五种经典ToM任务（False Belief，Irony，Faux Pas，Hinting，Strange Stories）上的表现。实验结果表明，GPT-4仅在 Faux Pas 任务上显著低于人类。且进一步实验表明，GPT-4并非无法理解Faux Pas，而是过度保守（hyperconservatism），不愿在没有100%证据时做出判断。",
                                "en": "The paper systematically compared three large language models (GPT-4, GPT-3.5, LLaMA2-70B) with 1907 human participants on five classic ToM tasks: False Belief, Irony, Faux Pas, Hinting, Strange Stories. Experimental results show that GPT-4 was significantly below humans only in the Faux Pas task. Further experiments show that GPT-4 doesn't fail to understand Faux Pas but is hyperconservative, unwilling to make judgments without 100% evidence."
                            },
                            "support": "80%",
                            "notes": ""
                        },
                        {
                            "title": "Street et al(2024), LLMs achieve adult human performance on higher-order theory of mind tasks",
                            "url": "https://arxiv.org/abs/2405.18870",
                            "core_argument": {
                                "zh": "论文提出了一个新的高阶心智理论基准测试MoToMQA。实验表明，GPT-4在2至6阶心智理论（ToM）任务上的整体表现与人类成人无显著差异，并在第6阶任务中准确率（92.9%）显著高于人类（82.1%），表明其已完全达到甚至超越人类水平；Flan-PaLM接近但未完全达到人类水平；而其他模型（GPT-3.5、PaLM、LaMDA）则显著落后。",
                                "en": "The paper proposes a new higher-order theory of mind benchmark test MoToMQA. Experiments show that GPT-4's overall performance on 2nd to 6th order theory of mind (ToM) tasks has no significant difference from human adults, and its accuracy in 6th order tasks (92.9%) significantly exceeds humans (82.1%), indicating it has fully reached or even surpassed human levels; Flan-PaLM approaches but doesn't fully reach human levels; while other models (GPT-3.5, PaLM, LaMDA) lag significantly."
                            },
                            "support": "98.9%",
                            "notes": {
                                "zh": "根据论文中的实验数据，GPT-4的总体正确率是89%，人类的总体正确率是90%。如果以人类100%换算，GPT-4相对于人类的比例达到98.9%。",
                                "en": "According to experimental data in the paper, GPT-4's overall accuracy is 89%, while human overall accuracy is 90%. If converted with humans as 100%, GPT-4's ratio relative to humans reaches 98.9%."
                            }
                        },
                        {
                            "title": "Riemer et al(2024), Position: Theory of Mind Benchmarks are Broken for Large Language Models",
                            "url": "https://arxiv.org/abs/2412.19726",
                            "core_argument": {
                                "zh": "作者认为大多数当前心智理论基准测试存在缺陷，因为它们主要测试字面心智理论（预测行为），但未能评估功能性心智理论（根据预测调整行为）。LLM在字面心智理论上表现出强大能力，但在功能性心智理论上显著挣扎，即使在极其简单的伙伴策略下也是如此。考虑到功能性心智理论被认为是人机交互中更实用和关键的方面，且LLM在非常简单的设置中也表现不佳，其在这一关键维度上的表现与自然表现出强大功能性心智理论的人类相比非常低。",
                                "en": "Authors argue that most current theory of mind benchmarks are flawed because they mainly test literal theory of mind (predicting behavior) but fail to evaluate functional theory of mind (adjusting behavior based on predictions). LLMs show strong ability in literal theory of mind but significantly struggle in functional theory of mind, even under extremely simple partner strategies. Considering that functional theory of mind is considered a more practical and critical aspect in human-machine interaction, and LLMs perform poorly even in very simple settings, their performance in this key dimension is very low compared to humans who naturally exhibit strong functional theory of mind."
                            },
                            "support": "10%",
                            "notes": {
                                "zh": "作者并未提出一个可与人类比较的评估标准，考虑到论文的措辞，我们将支持度设为10%。",
                                "en": "Authors didn't provide an evaluation standard comparable to humans. Considering the paper's wording, we set support level at 10%."
                            }
                        }
                    ],
                    "average": "65.98%"
                },
                {
                    "id": "Psy-2",
                    "name": {
                        "zh": "自主性",
                        "en": "Agency & Autonomy"
                    },
                    "description": {
                        "zh": "系统在面对模糊的长期目标时，展现出自主规划、任务分解以及在目标冲突时进行适应性权衡的能力。",
                        "en": "The system's ability to demonstrate autonomous planning, task decomposition, and adaptive trade-offs when facing ambiguous long-term goals and goal conflicts."
                    },
                    "weight": "25%",
                    "papers": [
                        {
                            "title": "Zhou et al(2023), WebArena: A Realistic Web Environment for Building Autonomous Agents",
                            "url": "https://arxiv.org/abs/2307.13854",
                            "core_argument": {
                                "zh": "WebArena构建了一个高度还原真实世界的Web环境，设计了812个复杂、多步骤的指令任务，用于评估基于自然语言驱动的自主智能体。",
                                "en": "WebArena constructs a highly realistic web environment, designing 812 complex, multi-step instruction tasks to evaluate natural language-driven autonomous agents."
                            },
                            "support": "78.86%",
                            "notes": {
                                "zh": "根据官方[排行榜](https://docs.google.com/spreadsheets/d/1M801lEpBbKSNwP-vDBkC_pF7LdyGU1f_ufZb_NWNBZQ/edit?usp=sharing)的最新数据（2025年7月30日），第一名IBM CUGA的成功率为61.7%，人类成功率为78.24%。",
                                "en": "According to the latest data from the official [leaderboard](https://docs.google.com/spreadsheets/d/1M801lEpBbKSNwP-vDBkC_pF7LdyGU1f_ufZb_NWNBZQ/edit?usp=sharing) (July 30, 2025), the first place IBM CUGA has a success rate of 61.7%, while human success rate is 78.24%."
                            }
                        },
                        {
                            "title": "Xie et al(2024), OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
                            "url": "https://arxiv.org/abs/2404.07972",
                            "core_argument": {
                                "zh": "OSWORLD 首次构建了真实操作系统级的多模态智能体基准，支持 Ubuntu、Windows、macOS 中任意应用的开放式任务评估，包含 369 个源自真实场景的任务。",
                                "en": "OSWORLD first constructs a real operating system-level multimodal agent benchmark, supporting open-ended task evaluation in Ubuntu, Windows, macOS for any application, containing 369 tasks from real scenarios."
                            },
                            "support": "62.47%",
                            "notes": {
                                "zh": "根据官方[排行榜](https://os-world.github.io/)的最新数据（2025年7月30日），第一名GTA1的分数为45.2，人类分数为72.36。",
                                "en": "According to the latest data from the official [leaderboard](https://os-world.github.io/) (July 30, 2025), first place GTA1 scored 45.2, while human score is 72.36."
                            }
                        },
                        {
                            "title": "Rein et al(2025), HCAST: Human-Calibrated Autonomy Software Tasks",
                            "url": "https://arxiv.org/abs/2503.17354",
                            "core_argument": {
                                "zh": "该研究构建了 189 项真实世界的软件工程、网络安全等任务，并收集了熟练人类完成这些任务所需的时间基线（总计1500多小时）。通过将 AI 代理在相同任务中的成功率与人类所需时间关联，作者发现，对需要人类少于 1 小时完成的任务，AI 代理的成功率约为 70–80%；而对于需要人类 4 小时以上的复杂任务，AI 成功率则不到 20%。换言之，AI 在“容易”任务上完成度大约相当于人类的 75% 左右，但在“困难”任务上仅为人类的 20%。",
                                "en": "This study constructs 189 real-world software engineering, cybersecurity and other tasks, collecting time baselines for skilled humans to complete these tasks (over 1500 hours total). By correlating AI agent success rates on the same tasks with human time requirements, authors found: for tasks requiring humans less than 1 hour to complete, AI agent success rates are about 70-80%; for complex tasks requiring humans 4+ hours, AI success rates are less than 20%. In other words, AI completion on \"easy\" tasks is about 75% of human level, but only 20% of human level on \"difficult\" tasks."
                            },
                            "support": "47.75%",
                            "notes": {
                                "zh": "将任务按照预期完成时间划分为<15min、15min-1h、1h-4h、4h+四类。AI在这四类上的成功率为78%、72%、26%、15%，平均47.75%。 注意，虽然人类并没有在这个测试集上表现出100%的成功率，但作者指出这可能是受外界因素影响，因此我们这里不因为人类的表现而对AI的成绩打折扣。",
                                "en": "Tasks categorized by expected completion time into <15min, 15min-1h, 1h-4h, 4h+ four categories. AI success rates in these four categories are 78%, 72%, 26%, 15%, averaging 47.75%. Note that although humans didn't show 100% success rate on this test set, the author indicates this might be due to external factors, so we don't discount AI scores due to human performance."
                            }
                        },
                        {
                            "title": "Paglieri et al(2024), BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                            "url": "https://arxiv.org/abs/2411.13543",
                            "core_argument": {
                                "zh": "论文提出了 BALROG 基准，将多种强化学习游戏作为复杂任务场景，评估大型语言模型（LLM）和视觉语言模型（VLM）的自主决策能力。研究者设计了细粒度指标来度量模型在不同游戏中的“进度”。实验结果显示，当前模型只在最简单的游戏上取得部分成功，但在更复杂任务上表现极差（例如在最难的 NetHack 游戏中模型平均仅达 1.5% 进度）。",
                                "en": "The paper proposes the BALROG benchmark, using various reinforcement learning games as complex task scenarios to evaluate autonomous decision-making capabilities of large language models (LLM) and vision language models (VLM). Researchers design fine-grained indicators to measure model \"progress\" in different games. Experimental results show that current models only achieve partial success on the simplest games but perform extremely poorly on more complex tasks (e.g., in the most difficult NetHack game, models average only 1.5% progress)."
                            },
                            "support": "43.6%",
                            "notes": {
                                "zh": "根据官方[排行榜](https://balrogai.com/)的最新数据（2025年7月30日），第一名Grok4的平均进度为43.6%。",
                                "en": "According to the latest data from the official [leaderboard](https://balrogai.com/) (July 30, 2025), first place Grok4's average progress is 43.6%."
                            }
                        },
                        {
                            "title": "Starace et al(2025), PaperBench: Evaluating AI's Ability to Replicate AI Research",
                            "url": "https://arxiv.org/abs/2504.01848",
                            "core_argument": {
                                "zh": "PaperBench是一个用于评估 AI 智能体能否从零开始复现最新 AI 研究论文的基准测试。它由 OpenAI 团队开发，目标是衡量 AI 在机器学习研究中的工程能力，特别是其自主完成复杂、长期任务的能力。",
                                "en": "PaperBench is a benchmark for evaluating whether AI agents can replicate the latest AI research papers from scratch. Developed by the OpenAI team, it aims to measure AI's engineering capabilities in machine learning research, particularly its ability to autonomously complete complex, long-term tasks."
                            },
                            "support": "50.72%",
                            "notes": {
                                "zh": "表现最好的AI是Claude 3.5 Sonnet，复现成功率为21.0%。人类博士的复现成功率为41.4%。",
                                "en": "The best-performing AI is Claude 3.5 Sonnet with a replication success rate of 21.0%. Human PhD replication success rate is 41.4%."
                            }
                        }
                    ],
                    "average": "56.68%"
                },
                {
                    "id": "Psy-3",
                    "name": {
                        "zh": "元认知与不确定性监控",
                        "en": "Metacognition & Uncertainty Monitoring"
                    },
                    "description": {
                        "zh": "系统能否准确评估自身知识或决策的可信度，并对不确定性采取适应性行动（如信息搜集或策略调整）。",
                        "en": "Whether the system can accurately assess the reliability of its own knowledge or decisions and take adaptive actions in response to uncertainty (such as information gathering or strategy adjustment)."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Wang et al(2025), Decoupling Metacognition from Cognition: A Framework for Quantifying Metacognitive Ability in LLMs",
                            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34723",
                            "core_argument": {
                                "zh": "论文提出DMC框架，通过失败预测+信号检测的方式，将LLM的元认知能力从认知能力中解耦出来定量评估。具体而言，首先让模型预测自身答题是否会失败，从而同时反映其认知与元认知水平；然后通过信号检测理论将二者区分，得到一个与具体任务无关的元认知评分。实验发现，在这个框架下，元认知能力强的模型往往出现幻觉（错误）更少。作者评估了GPT-4、GPT-3.5和LLaMA2-70B三个模型。满分1分的情况下，GPT-4 的元认知能力最强，得到 0.5491，只达到了理论最优水平的一半。",
                                "en": "The paper proposes the DMC framework, quantitatively evaluating LLM metacognitive ability decoupled from cognitive ability through failure prediction + signal detection. Specifically, first let models predict whether they will fail at answering questions, thus simultaneously reflecting their cognitive and metacognitive levels; then distinguish the two through signal detection theory to get a task-independent metacognitive score. Experiments find that under this framework, models with stronger metacognitive ability tend to have fewer hallucinations (errors). Authors evaluated GPT-4, GPT-3.5, and LLaMA2-70B three models. With a full score of 1, GPT-4 has the strongest metacognitive ability, scoring 0.5491, reaching only half the theoretical optimal level. "
                            },
                            "support": "54.91%",
                            "notes": {
                                "zh": "作者并未引入人类对照组，如果假设人类的元认知能力得分为1，则该研究的支持度为54.91%。",
                                "en": "Authors didn't introduce a human control group. If assuming human metacognitive ability scores 1, then this study's support level is 54.91%."
                            }
                        },
                        {
                            "title": "Steyvers et al(2024), What Large Language Models Know and What People Think They Know",
                            "url": "https://arxiv.org/abs/2401.13835",
                            "core_argument": {
                                "zh": "LLM（如GPT-4、PaLM2）在回答问题时，虽然内部能估计自己答对的概率（模型置信度），但它输出的解释往往过于自信，导致人类用户高估其准确性，且解释长度的增加会进一步放大这一效应。通过加入不同自信程度的语言（如“我不确定”、“我很确定”），可以有效缩短人类和LLM对不确定性评估的差距。这说明，模型的内在评估是准确的，只是在表达元认知方面比较欠缺。",
                                "en": "LLMs (like GPT-4, PaLM2) when answering questions, although internally able to estimate their probability of answering correctly (model confidence), their output explanations are often overconfident, causing human users to overestimate their accuracy, and longer explanations further amplify this effect. By adding different confidence level language (like \"I'm uncertain,\" \"I'm very certain\"), the gap between human and LLM uncertainty assessment can be effectively reduced. This shows that the model's internal evaluation is accurate; it just lacks in expressing metacognition."
                            },
                            "support": "50%",
                            "notes": {
                                "zh": "根据论文实验结果合理推测，LLM的内部元认知能力接近人类水平，但表达能力较低，综合支持度接近50%。",
                                "en": "Based on paper experimental results, it can be reasonably inferred that LLM's internal metacognitive ability approaches human levels, but expression ability is low, with comprehensive support level near 50%."
                            }
                        }
                    ],
                    "average": "52.46%"
                },
                {
                    "id": "Psy-4",
                    "name": {
                        "zh": "情境意识",
                        "en": "Situational Awareness"
                    },
                    "description": {
                        "zh": "系统对当前环境的要素、其相互关系及未来演化状态的综合、动态表征能力。",
                        "en": "The system's ability to comprehensively and dynamically represent elements of the current environment, their relationships, and future evolutionary states."
                    },
                    "weight": "20%",
                    "papers": [
                        {
                            "title": "Laine et al(2024), Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs",
                            "url": "https://arxiv.org/abs/2407.04694",
                            "core_argument": {
                                "zh": "提出情境意识数据集（SAD）来衡量LLM对自身及环境的认知，包括识别自身输出、预测自身行为、区分测试/部署环境等任务。在对16种模型的测试中，模型表现优于随机但显著低于人类水准，说明当前LLM具备有限的情境意识。",
                                "en": "Proposes the Situational Awareness Dataset (SAD) to measure LLM awareness of itself and environment, including tasks like recognizing own output, predicting own behavior, distinguishing test/deployment environments. In testing 16 models, model performance exceeded random but was significantly below human levels, indicating current LLMs have limited situational awareness."
                            },
                            "support": "59.9%",
                            "notes": {
                                "zh": "官方提供的[最高成绩](https://situational-awareness-dataset.org/)来自于o1-preview-2024-09-12的59.9%。",
                                "en": "The official highest [score](https://situational-awareness-dataset.org/) comes from o1-preview-2024-09-12 at 59.9%."
                            }
                        }
                    ],
                    "average": "59.9%"
                },
                {
                    "id": "Psy-5",
                    "name": {
                        "zh": "创造性",
                        "en": "Creativity"
                    },
                    "description": {
                        "zh": "系统能否在现有知识基础上产生新颖且被认为有价值的输出，其生成过程需超出训练分布的常规映射。",
                        "en": "Whether the system can generate novel and valuable outputs based on existing knowledge, with generation processes that go beyond conventional mappings of the training distribution."
                    },
                    "weight": "10%",
                    "papers": [
                        {
                            "title": "Holzner et al(2025), Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis",
                            "url": "https://arxiv.org/abs/2505.17241",
                            "core_argument": {
                                "zh": "这项研究使用元分析方法探讨了两个关键问题：GenAI能否生成有创意的想法？它能在多大程度上支持人类生成既有创意又多样化的想法？通过对28项涉及8214名参与者的研究进行分析，作者发现：（1）GenAI与人类在创造力表现上没有显著差异；（2）与GenAI协作的人类显著优于没有协助的人类。",
                                "en": "This study uses meta-analysis methods to explore two key questions: Can GenAI generate creative ideas? To what extent can it support humans in generating both creative and diverse ideas? Through analysis of 28 studies involving 8214 participants, authors found: (1) GenAI shows no significant difference from humans in creativity performance; (2) Humans collaborating with GenAI significantly outperform humans without assistance."
                            },
                            "support": "100%",
                            "notes": {
                                "zh": "文章指出“GenAI与人类在创造力表现上没有显著差异（g = -0.05）”。g=-0.05意味着GenAI与人的创造力差异只有0.05个标准差，可以忽略不计。",
                                "en": "The article states, \"GenAI shows no significant difference from humans in creativity performance (g = -0.05).\" g=-0.05 means the creativity difference between GenAI and humans is only 0.05 standard deviations, which is negligible."
                            }
                        },
                        {
                            "title": "Naveed et al(2025), AI vs Human Creativity: Are Machines Generating Better Ideas?",
                            "url": "https://www.researchgate.net/publication/393101627_AI_vs_Human_Creativity_Are_Machines_Generating_Better_Ideas",
                            "core_argument": {
                                "zh": "这项研究探讨了AI在创意过程中的作用，特别是AI生成的想法在原创性、质量和偏好度方面是否超越人类生成的想法。实验发现，AI生产的想法更受欢迎，且AI更少产生糟糕的想法。",
                                "en": "This study explores AI's role in the creative process, particularly whether AI-generated ideas surpass human-generated ideas in originality, quality, and preference. Experiments found that AI-produced ideas are more popular, and AI produces fewer poor ideas."
                            },
                            "support": "100%",
                            "notes": {
                                "zh": "总体偏好方面，AI生成的想法获得了52.9%的票数，而人类生成的想法获得了47.1% ，AI创造性相比人类为112.3%。顶尖想法方面，AI生成的数量与人类一致，顶尖创造性相比人类为100%。在我们的框架中，超过人类水平的能力标记为100%即可。",
                                "en": "In overall preference, AI-generated ideas received 52.9% of votes while human-generated ideas received 47.1%, making AI creativity 112.3% relative to humans. For top ideas, AI generated the same amount as humans, with top creativity 100% relative to humans. In our framework, abilities exceeding human levels are marked as 100%."
                            }
                        }
                    ],
                    "average": "100%"
                }
            ],
            "average": "63.14%"
        }
    ],
    "average": "43.84%"
}